{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 8205,
     "status": "ok",
     "timestamp": 1759403911096,
     "user": {
      "displayName": "Eneko Isturitz",
      "userId": "18248437753864671453"
     },
     "user_tz": -120
    },
    "id": "0GZSyqCdpMmN"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 6790,
     "status": "ok",
     "timestamp": 1759403917866,
     "user": {
      "displayName": "Eneko Isturitz",
      "userId": "18248437753864671453"
     },
     "user_tz": -120
    },
    "id": "vMuw6ej1pJaX"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "oYJxUanvrRd3"
   },
   "outputs": [],
   "source": [
    "models = {\n",
    "    'Shallow MLP': nn.Sequential(\n",
    "        nn.Linear(128*128, 64),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(64, 3)\n",
    "    ),\n",
    "    'Deep MLP': nn.Sequential(\n",
    "        nn.Linear(128*128, 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(256, 128),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(128, 64),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(64, 3)\n",
    "    ),\n",
    "    'Shallow CNN': nn.Sequential(\n",
    "        nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(32*64*64, 3) \n",
    "    ),\n",
    "    'Deep CNN': nn.Sequential(\n",
    "        nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(64*32*32, 128),  \n",
    "        nn.ReLU(),\n",
    "        nn.Linear(128, 3)\n",
    "    )\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "U96PW_nzp6_A"
   },
   "outputs": [],
   "source": [
    "class XRay_CNN(Dataset):\n",
    "    def __init__(self, path, dim):\n",
    "        self.path = path\n",
    "        self.normal = os.listdir(path + '/normal')\n",
    "        self.pneumonia = os.listdir(path + '/pneumonia')\n",
    "        self.tuberculosis = os.listdir(path + '/tuberculosis')\n",
    "        self.t = transforms.Compose([transforms.ToTensor(),\n",
    "                                     transforms.Grayscale(),\n",
    "                                     transforms.Resize([dim, dim])])\n",
    "    def __len__(self):\n",
    "        return len(self.normal) + len(self.pneumonia) + len(self.tuberculosis)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx < len(self.normal):\n",
    "            img = plt.imread(self.path + '/normal/' + self.normal[idx])\n",
    "            label = 0\n",
    "        elif idx < len(self.normal) + len(self.pneumonia):\n",
    "            img = plt.imread(self.path + '/pneumonia/' + self.pneumonia[idx - len(self.normal)])\n",
    "            label = 1\n",
    "        else:\n",
    "            img = plt.imread(self.path + '/tuberculosis/' + self.tuberculosis[idx - len(self.normal) - len(self.pneumonia)])\n",
    "            label = 2\n",
    "        img = self.t(img)\n",
    "        return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XRay_MLP(Dataset):\n",
    "    def __init__(self, path, dim):\n",
    "        self.path = path\n",
    "        self.normal = os.listdir(path + '/normal')\n",
    "        self.pneumonia = os.listdir(path + '/pneumonia')\n",
    "        self.tuberculosis = os.listdir(path + '/tuberculosis')\n",
    "        self.t = transforms.Compose([transforms.ToTensor(),\n",
    "                                     transforms.Grayscale(),\n",
    "                                     transforms.Resize([dim, dim]),\n",
    "                                     transforms.Lambda(lambda x: x.view(-1))])\n",
    "    def __len__(self):\n",
    "        return len(self.normal) + len(self.pneumonia) + len(self.tuberculosis)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx < len(self.normal):\n",
    "            img = plt.imread(self.path + '/normal/' + self.normal[idx])\n",
    "            label = 0\n",
    "        elif idx < len(self.normal) + len(self.pneumonia):\n",
    "            img = plt.imread(self.path + '/pneumonia/' + self.pneumonia[idx - len(self.normal)])\n",
    "            label = 1\n",
    "        else:\n",
    "            img = plt.imread(self.path + '/tuberculosis/' + self.tuberculosis[idx - len(self.normal) - len(self.pneumonia)])\n",
    "            label = 2\n",
    "        img = self.t(img)\n",
    "        return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Rx1eXiOFpzSA"
   },
   "outputs": [],
   "source": [
    "ds_train_CNN = XRay_CNN('dataset/train', 128)\n",
    "ds_val_CNN = XRay_CNN('dataset/val', 128)\n",
    "\n",
    "subset_size = 600\n",
    "train_subset_CNN, _ = random_split(ds_train_CNN, [subset_size, len(ds_train_CNN) - subset_size])\n",
    "\n",
    "subset_size = 300\n",
    "val_subset_CNN, _ = random_split(ds_val_CNN, [subset_size, len(ds_val_CNN) - subset_size])\n",
    "\n",
    "\n",
    "ds_train_MLP = XRay_MLP('dataset/train', 128)\n",
    "ds_val_MLP = XRay_MLP('dataset/val', 128)\n",
    "\n",
    "subset_size = 600\n",
    "train_subset_MLP, _ = random_split(ds_train_MLP, [subset_size, len(ds_train_MLP) - subset_size])\n",
    "\n",
    "subset_size = 300\n",
    "val_subset_MLP, _ = random_split(ds_val_MLP, [subset_size, len(ds_val_MLP) - subset_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "P_I3d1_VpquJ"
   },
   "outputs": [],
   "source": [
    "dl_train_CNN = DataLoader(train_subset_CNN, batch_size=32, shuffle=True)\n",
    "dl_val_CNN = DataLoader(val_subset_CNN, batch_size=32, shuffle=True)\n",
    "\n",
    "dl_train_MLP = DataLoader(train_subset_MLP, batch_size=32, shuffle=True)\n",
    "dl_val_MLP = DataLoader(val_subset_MLP, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "iXybAzcNpPr3"
   },
   "outputs": [],
   "source": [
    "def train_model(model, train_dl, val_dl, epochs=1000, lr=0.1):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    train_loss_history = []\n",
    "    train_acc_history = []\n",
    "    val_loss_history = []\n",
    "    val_acc_history = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "\n",
    "        for batch_x, batch_y in train_dl:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(batch_x)\n",
    "            loss = criterion(output, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            train_correct += (predicted == batch_y).sum().item()\n",
    "\n",
    "        train_loss_history.append(train_loss / len(train_dl))\n",
    "        train_acc_history.append(train_correct / len(train_dl.dataset))\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_x, batch_y in val_dl:\n",
    "                batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "                output = model(batch_x)\n",
    "                loss = criterion(output, batch_y)\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(output.data, 1)\n",
    "                val_correct += (predicted == batch_y).sum().item()\n",
    "\n",
    "        val_loss_history.append(val_loss / len(val_dl))\n",
    "        val_acc_history.append(val_correct / len(val_dl.dataset))\n",
    "\n",
    "        print(f\"Epoch {epoch}, Train Loss: {train_loss_history[-1]:.4f}, Train Acc: {train_acc_history[-1]:.4f}, Val Loss: {val_loss_history[-1]:.4f}, Val Acc: {val_acc_history[-1]:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "    return train_loss_history, train_acc_history, val_loss_history, val_acc_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alumno\\.conda\\envs\\py38ml\\lib\\site-packages\\torchvision\\transforms\\functional.py:154: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_numpy.cpp:212.)\n",
      "  img = torch.from_numpy(pic.transpose((2, 0, 1))).contiguous()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Train Loss: 1.7670, Train Acc: 0.4050, Val Loss: 1.0663, Val Acc: 0.6300\n",
      "Epoch 1, Train Loss: 0.9193, Train Acc: 0.6250, Val Loss: 0.8010, Val Acc: 0.6300\n",
      "Epoch 2, Train Loss: 0.7607, Train Acc: 0.6300, Val Loss: 0.7216, Val Acc: 0.6867\n",
      "Epoch 3, Train Loss: 0.6688, Train Acc: 0.6900, Val Loss: 0.6539, Val Acc: 0.7000\n",
      "Epoch 4, Train Loss: 0.6468, Train Acc: 0.7133, Val Loss: 0.6710, Val Acc: 0.7167\n",
      "Epoch 5, Train Loss: 0.6309, Train Acc: 0.7267, Val Loss: 0.7094, Val Acc: 0.6633\n",
      "Epoch 6, Train Loss: 0.6297, Train Acc: 0.7150, Val Loss: 0.6265, Val Acc: 0.7033\n",
      "Epoch 7, Train Loss: 0.5969, Train Acc: 0.7283, Val Loss: 0.5990, Val Acc: 0.7500\n",
      "Epoch 8, Train Loss: 0.5874, Train Acc: 0.7350, Val Loss: 0.5985, Val Acc: 0.7233\n",
      "Epoch 9, Train Loss: 0.5631, Train Acc: 0.7467, Val Loss: 0.5776, Val Acc: 0.7500\n"
     ]
    }
   ],
   "source": [
    "train_loss_history, train_acc_history, val_loss_history, val_acc_history = train_model(models['Shallow MLP'], dl_train_MLP, dl_val_MLP, epochs=10, lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Train Loss: 0.9038, Train Acc: 0.5500, Val Loss: 0.8593, Val Acc: 0.6367\n",
      "Epoch 1, Train Loss: 0.7401, Train Acc: 0.6683, Val Loss: 0.6355, Val Acc: 0.7367\n",
      "Epoch 2, Train Loss: 0.6409, Train Acc: 0.7200, Val Loss: 0.6600, Val Acc: 0.7067\n",
      "Epoch 3, Train Loss: 0.6397, Train Acc: 0.6950, Val Loss: 0.6707, Val Acc: 0.6433\n",
      "Epoch 4, Train Loss: 0.6192, Train Acc: 0.7333, Val Loss: 0.6224, Val Acc: 0.7433\n",
      "Epoch 5, Train Loss: 0.5684, Train Acc: 0.7517, Val Loss: 0.5703, Val Acc: 0.7300\n",
      "Epoch 6, Train Loss: 0.5465, Train Acc: 0.7683, Val Loss: 0.5691, Val Acc: 0.7333\n",
      "Epoch 7, Train Loss: 0.5416, Train Acc: 0.7633, Val Loss: 0.5863, Val Acc: 0.7200\n",
      "Epoch 8, Train Loss: 0.5743, Train Acc: 0.7367, Val Loss: 0.5889, Val Acc: 0.7267\n",
      "Epoch 9, Train Loss: 0.5397, Train Acc: 0.7700, Val Loss: 0.6276, Val Acc: 0.6700\n"
     ]
    }
   ],
   "source": [
    "train_loss_history, train_acc_history, val_loss_history, val_acc_history = train_model(models['Deep MLP'], dl_train_MLP, dl_val_MLP, epochs=10, lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Train Loss: 7.8537, Train Acc: 0.4517, Val Loss: 2.3542, Val Acc: 0.6200\n",
      "Epoch 1, Train Loss: 2.4361, Train Acc: 0.6200, Val Loss: 2.0840, Val Acc: 0.6400\n",
      "Epoch 2, Train Loss: 1.4594, Train Acc: 0.6650, Val Loss: 1.7049, Val Acc: 0.6833\n",
      "Epoch 3, Train Loss: 0.9919, Train Acc: 0.6900, Val Loss: 1.0946, Val Acc: 0.6900\n",
      "Epoch 4, Train Loss: 0.7797, Train Acc: 0.7317, Val Loss: 0.8727, Val Acc: 0.7133\n",
      "Epoch 5, Train Loss: 0.6753, Train Acc: 0.7383, Val Loss: 0.6681, Val Acc: 0.7233\n",
      "Epoch 6, Train Loss: 0.4845, Train Acc: 0.7867, Val Loss: 0.6046, Val Acc: 0.7067\n",
      "Epoch 7, Train Loss: 0.5527, Train Acc: 0.7767, Val Loss: 0.8574, Val Acc: 0.6633\n",
      "Epoch 8, Train Loss: 0.4224, Train Acc: 0.8200, Val Loss: 0.6943, Val Acc: 0.6900\n",
      "Epoch 9, Train Loss: 0.4900, Train Acc: 0.7767, Val Loss: 0.8565, Val Acc: 0.6933\n"
     ]
    }
   ],
   "source": [
    "train_loss_history, train_acc_history, val_loss_history, val_acc_history = train_model(models['Shallow CNN'], dl_train_CNN, dl_val_CNN, epochs=10, lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Train Loss: 1.2712, Train Acc: 0.4883, Val Loss: 0.7743, Val Acc: 0.7233\n",
      "Epoch 1, Train Loss: 0.7405, Train Acc: 0.6900, Val Loss: 0.7094, Val Acc: 0.6733\n",
      "Epoch 2, Train Loss: 0.6511, Train Acc: 0.7233, Val Loss: 0.6607, Val Acc: 0.7167\n",
      "Epoch 3, Train Loss: 0.6066, Train Acc: 0.7383, Val Loss: 0.5715, Val Acc: 0.7133\n",
      "Epoch 4, Train Loss: 0.5764, Train Acc: 0.7333, Val Loss: 0.5859, Val Acc: 0.7367\n",
      "Epoch 5, Train Loss: 0.5627, Train Acc: 0.7583, Val Loss: 0.6278, Val Acc: 0.7167\n",
      "Epoch 6, Train Loss: 0.5505, Train Acc: 0.7583, Val Loss: 0.5872, Val Acc: 0.7067\n",
      "Epoch 7, Train Loss: 0.4787, Train Acc: 0.7833, Val Loss: 0.5538, Val Acc: 0.7167\n",
      "Epoch 8, Train Loss: 0.4482, Train Acc: 0.7750, Val Loss: 0.5621, Val Acc: 0.7367\n",
      "Epoch 9, Train Loss: 0.4418, Train Acc: 0.7967, Val Loss: 0.5875, Val Acc: 0.7167\n"
     ]
    }
   ],
   "source": [
    "train_loss_history, train_acc_history, val_loss_history, val_acc_history = train_model(models['Deep CNN'], dl_train_CNN, dl_val_CNN, epochs=10, lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "256x256 deep CNN\n",
    "\n",
    "Epoch 0, Train Loss: 2.8587, Train Acc: 0.4200, Val Loss: 1.0547, Val Acc: 0.4500\n",
    "\n",
    "Epoch 1, Train Loss: 0.8792, Train Acc: 0.6733, Val Loss: 0.7679, Val Acc: 0.6400\n",
    "\n",
    "Epoch 2, Train Loss: 0.6183, Train Acc: 0.7433, Val Loss: 0.5706, Val Acc: 0.7400\n",
    "\n",
    "Epoch 3, Train Loss: 0.4802, Train Acc: 0.8100, Val Loss: 0.5828, Val Acc: 0.7500\n",
    "\n",
    "Epoch 4, Train Loss: 0.4373, Train Acc: 0.8200, Val Loss: 0.8224, Val Acc: 0.7400\n",
    "\n",
    "Epoch 5, Train Loss: 0.3681, Train Acc: 0.8467, Val Loss: 0.7928, Val Acc: 0.7700\n",
    "\n",
    "Epoch 6, Train Loss: 0.3037, Train Acc: 0.8633, Val Loss: 0.8480, Val Acc: 0.7200\n",
    "\n",
    "Epoch 7, Train Loss: 0.2364, Train Acc: 0.9100, Val Loss: 0.6293, Val Acc: 0.7500\n",
    "\n",
    "Epoch 8, Train Loss: 0.1749, Train Acc: 0.9233, Val Loss: 0.7078, Val Acc: 0.7000\n",
    "\n",
    "Epoch 9, Train Loss: 0.1641, Train Acc: 0.9300, Val Loss: 0.7481, Val Acc: 0.7800"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMbg/vfEtrmkmWwMXUe17WT",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:.conda-py38ml] *",
   "language": "python",
   "name": "conda-env-.conda-py38ml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
